# Construyendo un modelo similar al GPT desde cero con teoría detallada e implementación de código

## La era de los Transformadores

Los Transformadores están revolucionando el mundo de la inteligencia artificial. Esta poderosa arquitectura de red neuronal, introducida en 2017, se ha convertido rápidamente en la opción preferida para el procesamiento del lenguaje natural, la inteligencia artificial generativa y más. Con la ayuda de los transformadores, hemos visto la creación de productos de inteligencia artificial de vanguardia como BERT, GPT-x, DALL-E y AlphaFold, que están cambiando la forma en que interactuamos con el lenguaje y resolvemos problemas complejos como el plegamiento de proteínas. Y las posibilidades emocionantes no terminan ahí: los transformadores también están causando revuelo en el campo de la visión por computadora con el advenimiento de los Vision Transformers.

Antes de los Transformadores, el mundo de la IA estaba dominado por las redes neuronales convolucionales para la visión por computadora (VGGNet, AlexNet), las redes neuronales recurrentes para el procesamiento del lenguaje natural y problemas de secuenciación (LSTMs, GRUs) y las redes generativas adversarias (GANs) para la IA generativa. En 2017, las CNN y las RNN representaban el 10,2% y el 29,4%, respectivamente, de los documentos publicados sobre reconocimiento de patrones. Pero ya en 2021, los investigadores de Stanford llamaron a los transformadores "modelos fundamentales" porque los ven impulsando un cambio de paradigma en la IA.

Los Transformadores combinan algunos de los beneficios que tradicionalmente se ven en las redes neuronales convolucionales (CNN) y recurrentes (RNN).

Un beneficio que los Transformadores comparten con las CNN es la capacidad de procesar la entrada de manera jerárquica, con cada capa del modelo aprendiendo características cada vez más complejas. Esto permite que los Transformadores aprendan representaciones altamente estructuradas de los datos de entrada, similares a cómo las CNN aprenden representaciones estructuradas de imágenes.

Otro beneficio que los Transformadores comparten con las RNN es la capacidad de capturar dependencias entre elementos en una secuencia. A diferencia de las CNN, que procesan la entrada en una ventana de longitud fija, los Transformadores utilizan mecanismos de autoatención que permiten al modelo relacionar directamente diferentes elementos de entrada entre sí, independientemente de su distancia en la secuencia. Esto permite que los Transformadores capturen dependencias de largo alcance entre los elementos de entrada, lo que es particularmente importante para tareas como el procesamiento del lenguaje natural, donde es necesario considerar el contexto de una palabra para comprenderla con precisión.

Andrej Karpathy ha resumido el poder de los Transformadores como una computadora diferenciable general de propósito. Es muy poderoso en un pase hacia adelante, porque puede expresar una computación muy general. Los nodos almacenan vectores y estos nodos se miran entre sí y pueden ver lo que es importante para su cálculo en los otros nodos. En el paso hacia atrás es optimizable con las conexiones residuales, la normalización de capa y la atención softmax. Por lo tanto, se puede optimizar utilizando métodos de primer orden como el descenso de gradiente. Y por último, se puede ejecutar eficientemente en hardware moderno (GPUs), que prefiere mucho paralelismo.

El Transformador es una magnífica arquitectura de red neuronal porque es una computadora diferenciable de propósito general. es simultáneamente:

1. expresivo (en el pase hacia adelante)
2. optimizable (mediante retropropagación + descenso de gradiente)
3. eficiente (gráfico de cómputo de alto paralelismo)

Sorprendentemente, la arquitectura de Transformer es muy resistente con el tiempo. El Transformer que salió en 2017 es básicamente el mismo que el actual, excepto que reorganizas algunas de las normalizaciones de las capas. Los investigadores están haciendo que los conjuntos de datos de entrenamiento sean mucho más grandes, la evaluación mucho más grande, pero mantienen la arquitectura intacta, lo cual es notable.

## Attention is all you need

La nueva arquitectura se propuso en el artículo Attention Is All You Need, publicado en 2017. El artículo describía un nuevo tipo de arquitectura de red neuronal llamada Transformador, que se basa en mecanismos de autoatención a diferencia de las redes neuronales convolucionales o recurrentes tradicionales.

El artículo original describe una arquitectura de codificador-decodificador (encoder-decoder) para que Transformer resuelva el problema de la traducción.

En este artículo, sin embargo, nos centraremos en la arquitectura de solo decodificador (decoder), ya que construiremos un modelo similar a GPT, es decir, estamos resolviendo una tarea de completar oraciones en lugar de una tarea de traducción descrita en el documento.

Para la tarea de traducción, primero debe codificar el texto de entrada y tener una capa de atención-cruzada (cross-attention layer) con el texto traducido. En la finalización de oraciones solo es necesaria la parte del decodificador, así que centrémonos en ella.

## Encoder-decorder stack

Para empezar, recordemos la arquitectura codificador-decodificador (encoder-decoder). Los modelos de lenguaje podrían clasificarse aproximadamente en solo Encoder (BERT): solo tienen los bloques Encoders, solo Decoders (GPT-x, PaLM, OPT), solo los bloques decodificadores y Encoder-Decoder (BART, T0/T5). Esencialmente, la diferencia se reduce a la tarea que está tratando de resolver:

1. Las arquitecturas de solo Encoder (como BERT) resuelven la tarea de predecir la palabra enmascarada en una oración (masked word in a sentence). Entonces la atención puede ver todas las palabras antes y después de esta palabra enmascarada. Esto tiene un término "modelado de lenguaje enmascarado" ("masked language modelling"). La arquitectura del Encoder generalmente se usa para tareas de modelado de lenguaje que involucran Encoding de una secuencia de tokens de entrada y la producción de una representación de longitud fija (fixed-length), también conocida como Context Vector or Embedding, que resume la entrada. Este vector de contexto puede ser utilizado por una tarea posterior, como la traducción automática o el resumen de texto. El modelo BERT es un buen ejemplo.
2. Para las arquitecturas de Decoder y Encoder-Decoder, la tarea es predecir el siguiente token o conjunto de tokens, es decir, los tokens dados [0, ..., n-1] predicen n. Estas arquitecturas se utilizan para tareas de modelado de lenguaje que implican generar una secuencia de tokens de salida basados en un vector de contexto de entrada (input context vector).

En el artículo Attention is all you need, el Encoder y el Decoder están representados por seis capas (el número puede ser cualquiera, por ejemplo, en BERT hay 24 bloques Encoders).

Cada Encoder consta de dos capas: Self-Atention y Feed Forward Neural Network. Los datos de entrada del Encoder pasan primero por la capa de Self-Atention. Esta capa ayuda al Encoder a buscar otras palabras en la oración de entrada cuando codifica una palabra en particular. Los resultados de esta capa se envían a la capa completamente conectada (red neuronal de alimentación directa (Feed Forward Neural Network)).

A medida que el modelo procesa cada token (cada palabra en la secuencia de entrada), la capa de Self-Attention le permite buscar pistas en los otros tokens en la secuencia de entrada que pueden ayudar a mejorar la Encoding de esa palabra. En la capa totalmente conectada, la red neuronal de alimentación directa (Feed Forward Neural Network) no interactúa con otras palabras y, por lo tanto, se pueden ejecutar varias cadenas en paralelo a medida que pasan por esta capa. Esta es una de las principales características de Transformers, que les permite procesar todas las palabras del texto de entrada en paralelo.

[see Encoder stack img](https://habrastorage.org/r/w1560/getpro/habr/upload_files/112/b04/0dc/112b040dc5607959986bc0d48d3c6124.png)
