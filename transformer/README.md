# A Mathematical Analysis and From-Scratch Implementation of Transformer Model

# `TransformerModel.__init__(self, d_model: int, *args, **kwargs)`

## `self.__get_rand_embedding(self, input_text: str)`

This method generates the embedding for the input vector of the tokenized information $S_{n \times 1}$

$\mathbb{S}\rightarrow\mathbb{R}^{n \times d}$

Where:

- $\mathbb{S}$: is the space of all text strings
- $\mathbb{R}$: is the set of real numbers
- $n$ is the length of the input text (`len_input_text`)
- $d$ is the dimension of the embedding vectors (`d_model`)
- $\mathbb{R}^{n \times d}$: is the ${\times d}$ matrices with real-valued entries (elements).

${\mathbf{E}\in{R^{n \times d}}}$

Where:

- $E$: is the embedding matrix generated by using the `numpy.random.rand` function.
- ${R^{n \times d}}$: is the embedding matrix with random values between 0 and 1, where the shape is $n \times d$.

$
\mathbf{E} =
\begin{bmatrix}
    e_{1,1} & e_{1,2} & \cdots & e_{1,d} \\
    e_{2,1} & e_{2,2} & \cdots & e_{2,d} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    e_{n,1} & e_{n,2} & \cdots & e_{n,d}
\end{bmatrix}
$

With $e_{i,j}$ being random numbers betwen 0 and 1.

## `self.__get_positional_encoding(initial_embedding=initial_emb)`

This method generates the positional encoding matrix $\mathbf{PE}_{n \times d}$ for the input vector of the tokenized information $S_{n \times 1}$ where:

$
\mathbf{PE}_{n \times d} =
\begin{bmatrix}
    p_{1,1} & p_{1,2} & \cdots & p_{1,d} \\
    p_{2,1} & p_{2,2} & \cdots & p_{2,d} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    p_{n,1} & p_{n,2} & \cdots & p_{n,d}
\end{bmatrix}
$

$
\mathbf{p}_{j,k}(pos, i) =
\begin{cases}
\sin\left(\frac{pos}{10000^{i/d*{model}}}\right) &\text{si } i\ \text{even}\\
\cos\left(\frac{pos}{10000^{i/d*{model}}}\right) &\text{si } i\ \text{odd}
\end{cases}
$

Where:

- $pos$: is the position of the word in the sentence.
- $i$: is the dimension of the embedding vector.

Finally, the positional encoding matrix is added to the embedding matrix to obtain the final embedding matrix $\mathbf{E}_{n \times d}$.

$\mathbf{E'}_{n \times d}=E_{n \times d} + PE_{pos \times i}$

# `LinearLayer.__init__(self, input_dim: int, output_dim: int)`
