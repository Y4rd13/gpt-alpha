# A Mathematical Analysis and From-Scratch Implementation of Transformer Model

# `TransformerModel.__init__(self, d_model: int, *args, **kwargs)`

This class contains the implementation of the Transformer model.

## `self.__get_rand_embedding(self, input_text: str)`

This method generates the embedding for the input vector of the tokenized information $S_{n \times 1}$

$$\mathbb{S}\rightarrow\mathbb{R}^{n \times d}$$

Where:

- $\mathbb{S}$: is the space of all text strings
- $\mathbb{R}$: is the set of real numbers
- $n$ is the length of the input text (`len_input_text`)
- $d$ is the dimension of the embedding vectors (`d_model`)
- $\mathbb{R}^{n \times d}$: is the ${\times d}$ matrices with real-valued entries (elements).

$${\mathbf{E}\in{R^{n \times d}}}$$

Where:

- $E$: is the embedding matrix generated by using the `numpy.random.rand` function.
- ${R^{n \times d}}$: is the embedding matrix with random values between 0 and 1, where the shape is $n \times d$.

$$
\mathbf{E} =
\begin{bmatrix}
    e_{1,1} & e_{1,2} & \cdots & e_{1,d} \\
    e_{2,1} & e_{2,2} & \cdots & e_{2,d} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    e_{n,1} & e_{n,2} & \cdots & e_{n,d}
\end{bmatrix}
$$

With $e_{i,j}$ being random numbers betwen 0 and 1.

## `self.__get_positional_encoding(initial_embedding=initial_emb)`

This method generates the positional encoding matrix $\mathbf{PE}_{n \times d}$ for the input vector of the tokenized information $S_{n \times 1}$ where:

$$
\mathbf{PE}_{n \times d} =
\begin{bmatrix}
    p_{1,1} & p_{1,2} & \cdots & p_{1,d} \\
    p_{2,1} & p_{2,2} & \cdots & p_{2,d} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    p_{n,1} & p_{n,2} & \cdots & p_{n,d}
\end{bmatrix}
$$

$$
\mathbf{p}_{j,k}(pos, i) =
\begin{cases}
\sin\left(\frac{pos}{10000^{i/d*{model}}}\right) &\text{si } i\ \text{even}\\
\cos\left(\frac{pos}{10000^{i/d*{model}}}\right) &\text{si } i\ \text{odd}
\end{cases}
$$

Where:

- $pos$: is the position of the word in the sentence.
- $i$: is the dimension of the embedding vector.

Finally, the positional encoding matrix is added to the embedding matrix to obtain the final embedding matrix $\mathbf{E}_{n \times d}$.

$$\mathbf{E'}_{n \times d}=E_{n \times d} + PE_{pos \times i}$$

# `LinearLayer.__init__(self, input_dim: int, output_dim: int)`

This class implements a linear layer of a linear layer model. The linear layer class takes as input the dimensions of the layer as n and d so LinearLayer : $\mathbb{N}^2 \rightarrow \mathbb{R}^{n,d}$

<!-- ## `__init__(self, input_dim: int, output_dim: int)` First we initialize the `weight` and the scale of the linear layer.
The `weight` $\mathbf{W}_{m \times k}$ is initialized randomly using a normal distribution and the `scale` is initialized using the square root of the output dimension. The `scale` is used to normalize the output scores scaling factor $\sqrt{k}$ as follows:

$\mathbf{W}_{m \times k} \rightarrow \mathbf{W}_{m \times k} / \sqrt{k}$ -->

## `forward(self, x)`

This method implements the forward pass of the linear layer, which is a simple matrix multiplication between the input $X_{n \times d}$ and the weight $\mathbf{W}_{d \times k}$ to obtain the output. The output is then scaled by the `scale` of the linear layer.

$$\mathbb{R^{n \times m}} \rightarrow\mathbb{R^{n \times k}}$$

The function takes a tensor of size $n \times m$ and return a tensor of size $n \times k$, where $k$ is the output dimension layer.

<!-- $$\text{forward}(x) = \frac{1}{\sqrt{d_{out}}}xW$$ -->

$$\text{forward}(X_{n \times d}) = \frac{X_{n \times d} \cdot \mathbf{W}_{d \times k}}{\sqrt{d_{out}}}$$

Where:

- $X_{n \times d}$: is the input tensor of size $n \times m$ using a weight matrix $\mathbf{W}_{d \times k}$.
- $\mathbf{W}_{d \times k}$: is the randomly initialized using a normal distribution with mean 0 and standard deviation 1. And has a shape (dimension) of $m \times k$. Where $d_{in}$ denotes the input dimension of the layer (i.e., the number of neurons in the previous layer).
- $k$ is the number of features (dimensions) in the input tensor given by the number of neurons.
<!-- Verificar si es correcto que hable de la desviacion standard -->
- The scaling factor $\frac{1}{\sqrt{d_{out}}}$ is used to normalize the output scores and is equivalent to the standard deviation of the output when the input is randomly initialized.

<!-- $\frac{$X*{n \times d} \cdot $\mathbf{W}_{m \times k}}{X}$ -->

# `MultiHeadAttention(LinearLayer).__init__(self, len_input_text, d_model, *args, **kwargs)`

We start initializing the weights for the query, key and value using the LinearLayer class. The query, key and value are initialized with the same dimension of the embedding matrix $\mathbf{E}_{n \times d}$.

# `forward(self, x)`
